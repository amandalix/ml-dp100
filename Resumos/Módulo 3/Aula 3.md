# RealizaÃ§Ã£o de ajuste de HiperparÃ¢metros com o Az Machine Learning

> Bootcamp DP-100 DIO  >>>> [Microsoft Certification Challenge #3 DP-100](https://web.dio.me/track/d5adf7bc-330f-4c81-adc1-cac7e65bb151).

## Entender o ajuste de hiperparÃ¢metros.
---
O ajuste de hiperparÃ¢metro Ã© realizado por meio do treinamento de vÃ¡rios modelos, usando o mesmo algortimo e os mesmos dados de treinamento, mas valores de hiperparÃ¢metros diferentes.

Dentro do processamento do algoritmo, o modelo vai iterar pelos hiperparÃ¢metros (pesos) resultando em modelos distintos. Para algum deles o modelo vai ficar melhor.

No AutomatedML, automaticamente ele vai testando os melhores hiperparÃ¢metros.

## Usar um trabalho de varredura para ajuste de hiperparÃ¢meiro 
---

No Azure Machine Learning, vocÃª pode ajustar os hiperparÃ¢metros executando um trabalho de varredura.

1. Criar um script de treinamento para ajuste de hiperparÃ¢metro.
2. Configurar e executar um trabalho de varredura - sweep job
3. Monitorar e revusar trabalhos de varredura (filho)

> [Lab/09 - Perform hyperparameter tuning with a sweep job](https://microsoftlearning.github.io/mslearn-azure-ml/Instructions/09-Hyperparameter-tuning.html)


## Definir espaÃ§o de pesquisa
---
O conjunto de valores de hiperparÃ¢metro testado durante o ajuste de hiperparÃ¢metro Ã© conhecido como o espaÃ§o de pesquisa. A definiÃ§Ã£o do intervalo de possÃ­veis valores que podem ser escolhidos depende do tipo de hiperparÃ¢metro.

1. **HiperparÃ¢metros discretos**: alguns hiperparÃ¢metros exigem valores discretos â€” em outras palavras, vocÃª deve selecionar o valor em um determinado conjunto finito de possibilidades.

- NÃºmeros finitos de parÃ¢metros, sÃ³ itera sobre os nÃºmeros finitos individualmente.

2. **HiperparÃ¢metros contÃ­nuos**: alguns hiperparÃ¢metros sÃ£o contÃ­nuos â€” em outras palavras, vocÃª pode usar qualquer valor ao longo de uma escala, resultando em um nÃºmero infinito de possibilidades

Definir um espaÃ§o de pesquisa: para definir um espaÃ§o de pesquisa para ajuste de hiperparÃ¢metro, crie um dicionÃ¡rio com a expressÃ£o de parÃ¢metro apropriada para cada hiperparÃ¢metro nomeado

Faz a varredura em cima dos parÃ¢metros indicados e mostra o ranking do melhor filho que teve a melhor performance.

## Configurar um mÃ©todo de amostragem
---
Os valores especÃ­ficos usados em uma execuÃ§Ã£o de ajuste de hiperparÃ¢metro, ou trabalho de varredura, dependem do tipo de amostragem usada. HÃ¡ trÃªs mÃ©todos de amostragem principais disponÃ­veis no Azure Machine Learning:
1. **Amostragem de grade**: tenta todas as combinaÃ§Ãµes possÃ­veis. 
* Utilize todas as combinaÃ§Ãµes e executa sobre tudo
* Evitar um range grande de parÃ¢metros, deve-se usar uma polÃ­tica de bloqueio.
* Cria vÃ¡rios filhos, e mostra os resultados para todos.

2. **Amostragem aleatÃ³ria**: escolhe valores aleatoriamente no espaÃ§o de pesquisa 
    * Sobol: adiciona uma semente Ã  amostragem aleatÃ³ria para tornar os resultados reproduzÃ­veis

3. **Amostragem bayesiana**: escolhe novos valores com base nos resultados anteriores.
* Se for ruim, ele escolhe um parÃ¢metro que melhore o modelo com base no que foi executado anteriormente.

> Se refere a amostragem dos hiperparÃ¢metros e nÃ£o a amostragem dos dados.

## Configurar tÃ©rmino antecipado
---

Objetivo: nÃ£o perder tempo de processamento. Criar uma regra para tÃ©rmino antecipado, com base em nÃºmero de avaliaÃ§Ãµes, polÃ­ticas de encerramento ou verifica o desempenho, se nÃ£o estiver melhorando com base no modelo anterior ele para a varredura.

* Ao configurar um trabalho de varredura no Azure Machine Learning, vocÃª pode definir um nÃºmero mÃ¡ximo de avaliaÃ§Ãµes.

* Uma abordagem mais sofisticada pode ser parar um trabalho de varredura quando modelos mais recentes nÃ£o produzem resultados significativamente melhores.

* Para interromper um trabalho de varredura com base no desempenho dos modelos, vocÃª pode usar uma polÃ­tica de encerramento antecipado.

### Quando usar uma polÃ­tica de encerramento antecipado

Se vocÃª deve ou nÃ£o usar uma polÃ­tica de encerramento antecipado depende do espaÃ§o de pesquisa e do mÃ©todo de amostragem com o qual vocÃª estÃ¡ trabalhando.

Uma **polÃ­tica de encerramento antecipado** pode ser especialmente benÃ©fica ao trabalhar com hiperparÃ¢metros contÃ­nuos em seu espaÃ§o de pesquisa.

>Com o mÃ©todo de amostragem bayesiano, nÃ£o pode ter uma polÃ­tica de encerramento antecipado.

## PolÃ­tica de encerramento antecipado
---
HÃ¡ dois parÃ¢metros principais quando vocÃª opta por usar uma polÃ­tica de encerramento antecipado:

|*evauation_interval*|*delay_evaluation*|
|---|---|
|especifica em qual intervalo vocÃª deseja que a polÃ­tica seja avaliada|especifica quando comeÃ§ar a avaliar a polÃ­tica|


Os novos modelos podem continuar a ter um desempenho um pouco melhor que os modelos anteriores. Para determinar a extensÃ£o segundo a qual um modelo deve ter um desempenho melhor que o apresentado em avaliaÃ§Ãµes anteriores, hÃ¡ trÃªs opÃ§Ãµes de encerramento antecipado:
* **PolÃ­tica Bandit**
* **PolÃ­tica de Encerramento Mediana**
* **PolÃ­tica de seleÃ§Ã£o de truncamento**

### PolÃ­tica Bandit
---
VocÃª pode usar uma polÃ­tica do Bandit para interromper uma avaliaÃ§Ã£o se, atÃ© o momento, a mÃ©trica de desempenho de destino nÃ£o superar a melhor avaliaÃ§Ã£o por uma margem especificada.
> Python
```py
from azure.ai.ml.sweep import BanditPolicy


sweep_job.early_termination = 
    BanditPolicy(
        slack_amount = 0.2,
        delay_evaluation = 5,
        evaluation_interval = 1
    )
```

**ðŸ” ExplicaÃ§Ã£o de cada parÃ¢metro**

* **slack_amount = 0.2**: Esse parÃ¢metro define uma **margem de tolerÃ¢ncia** em relaÃ§Ã£o ao melhor desempenho atual.

    - Um experimento serÃ¡ interrompido se seu desempenho estiver **pior que 20% (0.2)** em relaÃ§Ã£o ao melhor desempenho observado atÃ© o momento.
    - Isso ajuda a **eliminar candidatos fracos mais rapidamente**, economizando recursos.

* **delay_evaluation = 5**: Define o nÃºmero de **execuÃ§Ãµes iniciais (ou *trial s*)** antes de comeÃ§ar a aplicar a polÃ­tica de interrupÃ§Ã£o.

    - Garante que todos os modelos tenham **tempo suficiente para apresentar resultados iniciais significativos**, antes de comparÃ¡-los e decidir interromper algum.

* **evaluation_interval = 1**: Indica que a polÃ­tica serÃ¡ aplicada a **cada execuÃ§Ã£o (trial)** a partir do momento em que `delay_evaluation` for atingido.
    - Ou seja, **apÃ³s 5 execuÃ§Ãµes**, o desempenho de cada nova execuÃ§Ã£o serÃ¡ avaliado e comparado com os melhores atÃ© entÃ£o â€” a **cada 1 nova execuÃ§Ã£o**.

Essa polÃ­tica Ã© usada para tornar o processo de *hyperparameter tuning* mais eficiente, **economizando tempo e recursos computacionais** ao interromper execuÃ§Ãµes que provavelmente nÃ£o levarÃ£o a bons resultados.

Itera o modelo atÃ© que os resultados nÃ£o estejam mais dentro dos parÃ¢metros.

### PolÃ­tica de Encerramento Mediana
---


Uma polÃ­tica de encerramento mediana abandona as avaliaÃ§Ãµes em que a mÃ©trica de desempenho de destino Ã© pior do que a mediana das mÃ©dias em execuÃ§Ã£o para todas as avaliaÃ§Ãµes.
> Python
```py
from azure.ai.ml.sweep import MediaStoppingPolicy

sweep_job.early_termination = 
    MedianStoppingPolicy(
        delay_evaluation = 5,
        evaluation_interval = 1
    )
```

**ðŸ§  ExplicaÃ§Ã£o dos parÃ¢metros:** `MedianStoppingPolicy`

ðŸ”¹ `sweep_job.early_termination = MedianStoppingPolicy(delay_evaluation = 5, evaluation_interval = 1)`

Essa linha define uma polÃ­tica de **interrupÃ§Ã£o antecipada** (early termination) baseada em mediana para experimentos de *hyperparameter tuning*.

ðŸ”¹ `delay_evaluation = 5`
Define o nÃºmero de **execuÃ§Ãµes (trials)** que devem ser completadas antes de comeÃ§ar a aplicar a polÃ­tica.

- Garante que cada experimento tenha **tempo suficiente para gerar dados significativos** antes de ser avaliado para possÃ­vel interrupÃ§Ã£o.

ðŸ”¹ `evaluation_interval = 1`
Indica com que frequÃªncia a polÃ­tica serÃ¡ aplicada apÃ³s o atraso inicial.

- Com valor `1`, a avaliaÃ§Ã£o ocorre **a cada nova execuÃ§Ã£o (trial)**.


**ðŸ“Œ O que faz a `MedianStoppingPolicy`?**

Essa polÃ­tica interrompe execuÃ§Ãµes **cujo desempenho esteja abaixo da mediana** das execuÃ§Ãµes anteriores no mesmo ponto de avaliaÃ§Ã£o.

- Ã‰ baseada em uma **comparaÃ§Ã£o estatÃ­stica**, e nÃ£o no melhor desempenho absoluto.
- Isso permite interromper experimentos **com baixo potencial de forma eficiente**, otimizando o uso de recursos computacionais.

Essa polÃ­tica Ã© ideal para manter o foco nos modelos mais promissores e acelerar o processo de busca pelos melhores hiperparÃ¢metros.



### PolÃ­tica de seleÃ§Ã£o de trucamento
---
Uma polÃ­tica de seleÃ§Ã£o de truncamento cancela X% das avaliaÃ§Ãµes com pior desempenho em cada intervalo de avaliaÃ§Ã£o com base no valor de `truncation_percentage` especificado para X.
> Python
```py
from azure.ai.ml.sweep import TruncationSelectionPolicy

sweep_job.early_terminatio = 
    TruncationSelectionPolicy(
        evaluation_interval = 1,
        truncation_percentage=20,
        delay_evaluation=4
    )
```
**ðŸš€ ExplicaÃ§Ã£o dos parÃ¢metros: `TruncationSelectionPolicy`**

Essa linha configura uma polÃ­tica de **interrupÃ§Ã£o antecipada** usando o mÃ©todo de **truncaÃ§Ã£o de seleÃ§Ãµes** para experimentos de *hyperparameter tuning*.


ðŸ”¹ `evaluation_interval = 1`
Define com que frequÃªncia os experimentos serÃ£o avaliados.

- Com valor `1`, a avaliaÃ§Ã£o acontece **a cada nova execuÃ§Ã£o (trial)** apÃ³s o tempo de espera inicial.


ðŸ”¹ `truncation_percentage = 20`
Determina a porcentagem de execuÃ§Ãµes a serem interrompidas.

- Nesse caso, os **20% dos experimentos com pior desempenho** serÃ£o parados durante cada avaliaÃ§Ã£o.


ðŸ”¹ `delay_evaluation = 4`
Especifica o nÃºmero de execuÃ§Ãµes (trials) que precisam ser completadas **antes de comeÃ§ar a aplicar a polÃ­tica**.

- Garante que haja **dados suficientes para uma avaliaÃ§Ã£o justa** antes de eliminar os piores resultados.


**ðŸ“Œ Como funciona a `TruncationSelectionPolicy`?**

ApÃ³s esperar `4` execuÃ§Ãµes, o sistema comeÃ§a a avaliar o desempenho **a cada nova execuÃ§Ã£o**.

- Durante cada avaliaÃ§Ã£o, **os 20% dos experimentos com pior desempenho** sÃ£o **interrompidos**.
- Assim, o processo foca rapidamente nos experimentos mais promissores, economizando **tempo e recursos computacionais**.


Essa polÃ­tica Ã© muito Ãºtil para acelerar o *hyperparameter tuning*, eliminando candidatos fracos em vÃ¡rias rodadas ao longo do processo!

>Slide: [MÃ³dulo 4 - Otimizar o Treinamento de Modelo no Azure Machine Learning.pptx](https://web.dio.me/track/microsoft-certification-challenge-dp-100/course/realizacao-de-ajuste-de-hiperparametros-com-o-azure-machine-learning/learning/12cc8096-6b34-44a4-b29f-aeef9bac001d?autoplay=1&back=%2Ftrack%2Fmicrosoft-certification-challenge-dp-100#:~:text=Slide%3A%C2%A0M%C3%B3dulo%204%20%2D%20Otimizar%20o%20Treinamento%20de%20Modelo%20no%20Azure%20Machine%20Learning.pptx)